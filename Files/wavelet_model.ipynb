{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary functions\n",
    "\n",
    "import os\n",
    "import random\n",
    "import Augmentor\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "tf.config.run_functions_eagerly(True)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling2D,\n",
    "    Activation,\n",
    "    Conv2DTranspose,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from wavetf._wavetf import WaveTFFactory\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, BatchNormalization, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/fversaci/WaveTF.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import layers, Sequential\n",
    "from wavetf import WaveTFFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletBlock(layers.Layer):\n",
    "  def __init__(self, input_channels, wavelet, interpolation, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.input_channels = input_channels\n",
    "    self.wavelet = wavelet\n",
    "    self.interpolation = interpolation\n",
    "    self.wavelet_transform = WaveTFFactory.build(wavelet)\n",
    "    self.cA = [layers.UpSampling2D(size = (2,2), interpolation = interpolation),\n",
    "               layers.DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same'),\n",
    "               layers.BatchNormalization(),\n",
    "               layers.Activation('relu')\n",
    "              ]\n",
    "    self.cH = [layers.UpSampling2D(size = (2,2), interpolation = interpolation),\n",
    "               layers.DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same'),\n",
    "               layers.BatchNormalization(),\n",
    "               layers.Activation('relu')\n",
    "              ]\n",
    "    self.cV = [layers.UpSampling2D(size = (2,2), interpolation = interpolation),\n",
    "               layers.DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same'),\n",
    "               layers.BatchNormalization(),\n",
    "               layers.Activation('relu')\n",
    "              ]\n",
    "    self.cD = [layers.UpSampling2D(size = (2,2), interpolation = interpolation),\n",
    "               layers.DepthwiseConv2D(kernel_size = (3,3), strides = (1,1), padding = 'same'),\n",
    "               layers.BatchNormalization(),\n",
    "               layers.Activation('relu')\n",
    "              ]\n",
    "    self.concat = layers.Concatenate(axis = -1)\n",
    "    self.linear = [layers.Conv2D(filters = input_channels, kernel_size = (1,1), strides = (1,1), padding = 'same'),\n",
    "                   layers.BatchNormalization()\n",
    "                  ]\n",
    "    self.batchnorm = [layers.BatchNormalization(),\n",
    "                      layers.Activation('relu')]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z = self.wavelet_transform(inputs)\n",
    "\n",
    "    cA = z[:,:,:,:self.input_channels]\n",
    "    cV = z[:,:,:,self.input_channels:2*self.input_channels]\n",
    "    cH = z[:,:,:,2*self.input_channels:3*self.input_channels]\n",
    "    cD = z[:,:,:,3*self.input_channels:]\n",
    "\n",
    "    for layer in self.cA:\n",
    "      cA = layer(cA)\n",
    "    for layer in self.cV:\n",
    "      cV = layer(cV) \n",
    "    for layer in self.cH:\n",
    "      cH = layer(cH) \n",
    "    for layer in self.cD:\n",
    "      cD = layer(cD) \n",
    "    \n",
    "    out = self.concat([cA,cV,cH,cD])\n",
    "\n",
    "    for layer in self.linear:\n",
    "      out = layer(out)\n",
    "    \n",
    "    if inputs.shape[1] != out.shape[1] or inputs.shape[2] != out.shape[2]:\n",
    "        out = tf.image.resize(out, (inputs.shape[1], inputs.shape[2]), method=self.interpolation)\n",
    "    final = out + inputs\n",
    "\n",
    "    for layer in self.batchnorm:\n",
    "      final = layer(final)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mnist_model(wavelet, interpolation):\n",
    "  model = Sequential([layers.Input(shape = (28,28, 1), name = 'input'),\n",
    "                      layers.RandomRotation(factor = 10/360, name = 'random_rotation'),\n",
    "                      layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', name = 'conv2d_1'),\n",
    "                      layers.BatchNormalization(name = 'conv2d_1_bn'),\n",
    "                      layers.Activation('relu', name = 'conv2d_1_relu'),\n",
    "                      WaveletBlock(input_channels = 32, wavelet = wavelet, interpolation = interpolation, name = 'wavelet_block_1'),\n",
    "                      layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), padding = 'same', name = 'conv2d_2'),\n",
    "                      layers.BatchNormalization(name = 'conv2d_2_bn'),\n",
    "                      layers.Activation('relu', name = 'conv2d_2_relu'),\n",
    "                      WaveletBlock(input_channels = 64, wavelet = wavelet, interpolation = interpolation, name = 'wavelet_block_2'),\n",
    "                      layers.Conv2D(filters = 128, kernel_size = (3,3), strides = (1,1), padding = 'same', name = 'conv2d_3'),\n",
    "                      layers.BatchNormalization(name = 'conv2d_3_bn'),\n",
    "                      layers.Activation('relu', name = 'conv2d_3_relu'),\n",
    "                      layers.GlobalAveragePooling2D(name = 'avgpooling'),\n",
    "                      layers.Dropout(0.4, name = 'dropout'),\n",
    "                      layers.Dense(units = 10, activation = 'softmax', name = 'predictions')],\n",
    "                     name = f'{wavelet}_{interpolation}_model')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_mnist_model('haar', 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the directories\n",
    "implant_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Implant Xrays'\n",
    "normal_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Normal Xrays copy'\n",
    "\n",
    "# Define the parameters\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "\n",
    "# Initialize the ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    dtype=np.uint8,\n",
    "    validation_split=0.2  # Adjust the validation split as needed\n",
    ")\n",
    "\n",
    "# Create the generator for training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='grayscale',\n",
    "    subset='training',  # Use 'validation' for validation generator\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    seed=42  # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create the generator for validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='grayscale',\n",
    "    subset='validation',\n",
    "    shuffle=False,  # Do not shuffle the data for validation\n",
    "    seed=42  # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "# Convert generators to datasets\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, img_width, img_height, 1], [None])\n",
    ")\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, img_width, img_height, 1], [None])\n",
    ")\n",
    "\n",
    "# Prefetch the data for training and validation\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define directories\n",
    "implant_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Implant Xrays'\n",
    "normal_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Normal Xrays copy'\n",
    "\n",
    "# Define parameters\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def preprocess_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "    if img is None:\n",
    "        print(f\"Error loading image: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = cv2.resize(img, (img_width, img_height))  # Resize\n",
    "    img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "\n",
    "# Function to load images from directory and apply preprocessing\n",
    "def load_images_from_directory(directory):\n",
    "    file_paths = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in file_paths:\n",
    "        img = preprocess_image(file_path)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(1 if 'implant' in file_path.lower() else 0)  # Assuming 'implant' images are positive\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load and preprocess training images\n",
    "train_images, train_labels = load_images_from_directory(implant_xrays_dir)\n",
    "normal_train_images, normal_train_labels = load_images_from_directory(normal_xrays_dir)\n",
    "train_images = np.concatenate([train_images, normal_train_images], axis=0)\n",
    "train_labels = np.concatenate([train_labels, normal_train_labels], axis=0)\n",
    "\n",
    "# Shuffle training data\n",
    "shuffle_indices = np.random.permutation(len(train_images))\n",
    "train_images = train_images[shuffle_indices]\n",
    "train_labels = train_labels[shuffle_indices]\n",
    "\n",
    "# Load and preprocess validation images\n",
    "validation_images, validation_labels = load_images_from_directory(implant_xrays_dir)\n",
    "normal_validation_images, normal_validation_labels = load_images_from_directory(normal_xrays_dir)\n",
    "validation_images = np.concatenate([validation_images, normal_validation_images], axis=0)\n",
    "validation_labels = np.concatenate([validation_labels, normal_validation_labels], axis=0)\n",
    "\n",
    "# Convert NumPy arrays to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "\n",
    "# Shuffle and batch train dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch validation dataset\n",
    "validation_dataset = validation_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract a batch from the generator\n",
    "images, labels = next(train_generator)\n",
    "\n",
    "# Check the shape of an individual image\n",
    "print(\"Shape of an individual image:\", images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images[0][0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = train_dataset,\n",
    "                        initial_epoch = 0,\n",
    "                        epochs = 2, \n",
    "                        validation_data = validation_dataset\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary functions\n",
    "\n",
    "import os\n",
    "import random\n",
    "import Augmentor\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# Se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data_folder = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Normal Xrays'\n",
    "implant_data_folder = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Implant Xrays'\n",
    "\n",
    "# Get a list of all files in the data folder\n",
    "normal_files = os.listdir(normal_data_folder)\n",
    "implant_files = os.listdir(implant_data_folder)\n",
    "\n",
    "normal_files = [file for file in normal_files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "implant_files = [file for file in implant_files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "normal_image_list = []\n",
    "for image_file in normal_files:\n",
    "    image_path = os.path.join(normal_data_folder, image_file)\n",
    "    img = Image.open(image_path)\n",
    "    normal_image_list.append(img)\n",
    "\n",
    "implant_image_list = []\n",
    "for image_file in implant_files:\n",
    "    image_path = os.path.join(implant_data_folder, image_file)\n",
    "    img = Image.open(image_path)\n",
    "    implant_image_list.append(img)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_normal = Augmentor.Pipeline('/Users/aditya/Desktop/My Computer/DDP/New/Data/Normal Xrays')\n",
    "p_normal.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "p_normal.zoom(probability=0.3, min_factor=1.1, max_factor=1.6)\n",
    "\n",
    "p_normal.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_implant = Augmentor.Pipeline('/Users/aditya/Desktop/My Computer/DDP/New/Data/Implant Xrays')\n",
    "p_implant.rotate(probability=0.7, max_left_rotation=20, max_right_rotation=20)\n",
    "p_implant.zoom(probability=0.5, min_factor=0.8, max_factor=1.6)\n",
    "\n",
    "p_implant.sample(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implant_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Implant Xrays'\n",
    "normal_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Normal Xrays'\n",
    "\n",
    "# Define image dimensions and other parameters\n",
    "img_width, img_height = 150, 150  \n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Adjust the validation split as needed\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Use 'validation' for validation generator\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('xray_classifier_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = (history.history['loss'])\n",
    "val_loss = (history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.title(\"Loss Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "\n",
    "plt.legend(['Train_Loss','Val_Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = (history.history['accuracy'])\n",
    "val_acc = (history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.title(\"Accuracy Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "\n",
    "plt.legend(['Train_acc','Val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('xray_classifier_model.h5')  # Use the actual path to your saved model\n",
    "img_arr = []\n",
    "\n",
    "for i in range(9):\n",
    "    image_path = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Implant Xrays/' + str(i) +'.png'  \n",
    "    img = image.load_img(image_path, target_size=(150, 150))  \n",
    "    img_arr.append(img)\n",
    "\n",
    "for i in range(9):\n",
    "    image_path = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Normal Xrays/' + str(i) +'.png'  \n",
    "    img = image.load_img(image_path, target_size=(150, 150))  \n",
    "    img_arr.append(img)\n",
    "\n",
    "\n",
    "img_arr = random.sample(img_arr, len(img_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in img_arr:\n",
    "    \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalize the pixel values\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    if prediction[0][0] >= 0.5:\n",
    "        plt.title(\"Predicted: Normal X-ray\")\n",
    "    else:\n",
    "        plt.title(\"Predicted: Implant X-ray\")\n",
    "    \n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/aditya/Desktop/My Computer/DDP/New/knee_dataset3/Osteoporosis Knee X-ray/normal/N2.JPEG'  # Replace with the actual path to your image\n",
    "# image_path = '/Users/aditya/Desktop/My Computer/DDP/New/knee_dataset2/MedicalExpert-I/0Normal/NormalG0 (145).png'\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(image_path, target_size=(150, 150))  # Adjust target_size as needed\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize the pixel values\n",
    "\n",
    "# Make predictions\n",
    "plt.imshow(img)\n",
    "prediction = model.predict(img_array)\n",
    "print(prediction)\n",
    "# The model outputs a probability. If the probability is close to 0, it's likely a normal X-ray;\n",
    "# if it's close to 1, it's likely an implant X-ray.\n",
    "if prediction[0, 0] >= 0.5:\n",
    "    print(\"Predicted: Normal X-ray\")\n",
    "else:\n",
    "    print(\"Predicted: Implant X-ray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Dataset Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implant_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Implant Xrays'\n",
    "normal_xrays_dir = '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented/Normal Xrays copy'\n",
    "\n",
    "# Define image dimensions and other parameters\n",
    "img_width, img_height = 150, 150  \n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Adjust the validation split as needed\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Use 'validation' for validation generator\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/aditya/Desktop/My Computer/DDP/New/Data/Augmented',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
